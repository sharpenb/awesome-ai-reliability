# üåü Awesome ML Reliability üåü

![Awesome](https://awesome.re/badge.svg) ![MIT License](https://img.shields.io/badge/license-MIT-brightgreen)

A curated list of resources about Machine Learning (ML) reliability. It covers trustworthiness of ML systems by including resources on the topics listed below and more!

### Topics Summary üé®

| Topic            | Description                                    | Badge Example                                          |
|-------------------|------------------------------------------------|-------------------------------------------------------|
| **Uncertainty**   | Quantifying and managing uncertainty           | ![Uncertainty](https://img.shields.io/badge/Uncertainty-orange) |
| **Bayesian**      | Bayesian approaches to ML                      | ![Bayesian](https://img.shields.io/badge/Bayesian-lime) |
| **Robustness**    | Ensuring robustness to various conditions      | ![Robustness](https://img.shields.io/badge/Robustness-green) |
| **Attack**       | Exploring adversarial attacks                  | ![Attack](https://img.shields.io/badge/Attack-red) |
| **Defense**       | Defense mechanisms against vulnerabilities     | ![Defense](https://img.shields.io/badge/Defense-blue) |
| **Privacy**       | Addressing privacy concerns in ML systems      | ![Privacy](https://img.shields.io/badge/Privacy-purple) |
| **Fairness**      | Ensuring fairness and equity in AI             | ![Fairness](https://img.shields.io/badge/Fairness-teal) |
| **Explainability**| Improving interpretability of AI systems       | ![Explainability](https://img.shields.io/badge/Explainability-cyan) |

If you find this list helpful, give it a ‚≠ê on GitHub, share it, and feel free to contribute by submitting a pull request or issue!

---

## Table of Contents
- [Facts/Numbers üìä](#factsnumbers-üìä)
- [Tools üõ†Ô∏è](#tools-üõ†Ô∏è)
- [Newspaper Articles üì∞](#newspaper-articles-üì∞)
- [Blog Articles üóíÔ∏è](#blog-articles-üóíÔ∏è)
- [Research Articles üïå](#research-articles-üïå)
- [Books üìö](#books-üìö)
- [Lectures üéì](#lectures-üéì)
- [People üßë‚Äçüíª](#people-üßë‚Äçüíª)

---

## Facts üìä
- **Hundreds of millions of people now interact with**: Number of people interacting with AI models ([Source](https://arxiv.org/abs/2405.01470), 2024, [Source](https://arxiv.org/abs/2309.11998), 2024)
- **80%**: Percentage of ML projects that fail due to issues with data quality and system reliability ([Source](https://www.tomshardware.com/tech-industry/artificial-intelligence/research-shows-more-than-80-of-ai-projects-fail-wasting-billions-of-dollars-in-capital-and-resources-report), 2024).
- **60%**: Percentage of businesses using AI but which aren't developing ethical AI policies ([Source](https://www.venasolutions.com/blog/ai-statistics), 2024).
- **10,000+**: Research papers on ML robustness ([Source](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html), 2024).

---

## Tools üõ†Ô∏è
- **[Uncertainty Baselines](https://github.com/google/uncertainty-baselines)**: High-quality implementations of standard and SOTA methods on a variety of tasks.
- **[Uncertainty Toolbox](https://uncertainty-toolbox.github.io/)**: A Python toolbox for predictive uncertainty quantification, calibration, metrics, and visualization.
- **[RobustBench](https://robustbench.github.io/)**: A standardized benchmark for adversarial robustness.
- **[TorchMetrics](https://lightning.ai/docs/torchmetrics/stable/)**: A collection of 100+ PyTorch metrics implementations and an easy-to-use API to create custom metrics.
- **[Cleanlab](https://github.com/cleanlab/cleanlab)**: A Python library for finding and fixing label errors in datasets.
- **[DeepChecks](https://github.com/deepchecks/deepchecks)**: Comprehensive library for testing ML systems and datasets.

---

## Newspaper Articles üì∞
- *"[Research shows more than 80% of AI projects fail, wasting billions of dollars in capital and resources: Report
](https://www.tomshardware.com/tech-industry/artificial-intelligence/research-shows-more-than-80-of-ai-projects-fail-wasting-billions-of-dollars-in-capital-and-resources-report)"* (2024) - [Tom's Hardware](https://www.tomshardware.com/)
- *"[80 AI Statistics Shaping Business in 2024](https://www.venasolutions.com/blog/ai-statistics)"* (2024) - [Vena Solutions](https://www.venasolutions.com/)
- *"[OopsGPT](https://www.theatlantic.com/technology/archive/2024/07/searchgpt-openai-error/679248/?)"* (2024) - [The Atlantic](https://www.theatlantic.com/world/)

---

## Blog Articles üìù
- *"[A Complete List of All (arXiv) Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html)"* (Maintained) ![Robustness](https://img.shields.io/badge/Robustness-green) ![Attack](https://img.shields.io/badge/Attack-red) ![Defense](https://img.shields.io/badge/Defense-blue) - [Nicholas Carlini](https://nicholas.carlini.com/)
- *"[Adversarial Machine Learning Reading List](https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html)"* (2019) ![Robustness](https://img.shields.io/badge/Robustness-green) ![Attack](https://img.shields.io/badge/Attack-red) ![Defense](https://img.shields.io/badge/Defense-blue) - [Nicholas Carlini](https://nicholas.carlini.com/)

---

## Research Articles üìÑ
- *"[AI generates covertly racist decisions about people based on their dialect](https://www.nature.com/articles/s41586-024-07856-5)"* (2024) - [Nature](https://www.nature.com/)
- *"Adversarial Robustness for Deep Learning"* (2019) [Robustness, Security] - [ArXiv](https://arxiv.org/abs/1905.01065)
- *"Fairness and Robustness in AI Systems"* (2020) [Fairness, Robustness, Ethics] - [Nature Machine Intelligence](https://www.nature.com/natmachintell)
- *"Uncertainty Quantification in ML"* (2021) [Uncertainty, Reliability] - [IEEE Transactions on Neural Networks and Learning Systems](https://ieeexplore.ieee.org)
- *"Exploring Bayesian Methods in Uncertainty Quantification"* (2021) [Uncertainty, Bayesian Methods] - [Proceedings of Machine Learning Research](https://proceedings.mlr.press)
- *"Energy-based Models for Out-of-Distribution Detection"* (2022) [Robustness, OOD Detection] - [NeurIPS](https://neurips.cc)
- *"Graph Neural Networks: Robustness Under Adversarial Settings"* (2022) [Graph Learning, Robustness] - [ICLR](https://iclr.cc)

---

## Books üìö
- *"Reliable Machine Learning"* (2020) [Reliability, Engineering] by Erik Jones
- *"Trustworthy AI"* (2021) [Ethics, Trustworthiness] by Beena Ammanath
- *"Robust Machine Learning in Python"* (2022) [Robustness, Tools] by Sarah Bird
- *"Uncertainty in Deep Learning: A Practical Guide"* (2023) [Uncertainty, Methods] by Yarin Gal
- *"Explainable AI and its Reliability"* (2022) [Explainability, Reliability] by Christoph Molnar

---

## Lectures üéì
- **"Reliability in AI: Challenges and Solutions"** [Reliability, Challenges] - [MIT OpenCourseWare](https://ocw.mit.edu)
- **"Robustness in Machine Learning"** [Robustness, Techniques] - [Stanford Online](https://online.stanford.edu)
- **"Trustworthy AI Systems"** [Trustworthiness, Systems] - [DeepMind YouTube](https://www.youtube.com)
- **"Uncertainty Estimation and Trust in ML"** [Uncertainty, Estimation] - [Cambridge University YouTube](https://www.youtube.com)

---

## People üßë‚Äçüíª
- **Bertrand Charpentier** - Working in academia and industry on Bayesian deep learning and uncertainty estimation. [LinkedIn](https://www.linkedin.com/in/bertrand-charpentier/)
- **Stephan G√ºnnemann** - Working in academia on robust machine learning and graph neural networks. [Website](https://www.professoren.tum.de/guennemann-stephan)
- **Eyke H√ºllermeier** - Working in academia on uncertainty quantification and decision theory. [Website](https://www.uni-paderborn.de/en/person/16348)
- **Eric Nalisnick** - Working in academia on Bayesian deep learning and uncertainty estimation. [Website](https://www.cs.nott.ac.uk/~ean/)
- **Andrew Ng** - Working in academia and industry on online education and ML reliability. [Twitter](https://twitter.com/andrewyng) | [LinkedIn](https://www.linkedin.com/in/andrewyng/) | [Website](https://www.andrewng.org/)
- **Fei-Fei Li** - Working in academia on AI ethics and vision. [Twitter](https://twitter.com/feifeili) | [LinkedIn](https://www.linkedin.com/in/feifei-li/) | [Website](https://profiles.stanford.edu/fei-fei-li)
- **Timnit Gebru** - Working in industry on fairness and ethical AI. [Twitter](https://twitter.com/timnitGebru) | [Website](https://www.dair-institute.org/)
- **Ian Goodfellow** - Working in industry on adversarial robustness and ML security. [LinkedIn](https://www.linkedin.com/in/ian-goodfellow/) | [Website](https://www.iangoodfellow.com/)
- **Yarin Gal** - Working in academia on Bayesian methods and predictive uncertainty. [Twitter](https://twitter.com/yarin_gal) | [Website](https://yarin-gal.com/)
- **Zoubin Ghahramani** - Working in industry and academia on probabilistic models and Bayesian methods. [Twitter](https://twitter.com/zoubinghahramani) | [Website](https://mlg.eng.cam.ac.uk/zoubin/)
- **Balaji Lakshminarayanan** - Working in industry on deep ensembles and predictive uncertainty. [LinkedIn](https://www.linkedin.com/in/balaji-lakshminarayanan/) | [Website](https://balajiln.github.io/)
- **Dan Hendrycks** - Working in academia on robustness to distribution shifts and ML reliability. [Website](https://hendrycks.com/)
- **Alexander Amini** - Working in academia on evidential learning and autonomous systems. [Website](https://people.csail.mit.edu/amini/)
- **Dario Amodei** - Working in industry on AI alignment and safety. [LinkedIn](https://www.linkedin.com/in/darioamodei/)
- **Nicholas Carlini** - Working in industry on adversarial robustness and secure ML. [Website](https://nicholas.carlini.com/)
- **Chelsea Finn** - Working in academia on meta-learning and reliability in robotics. [Twitter](https://twitter.com/chelseabfinn) | [Website](https://cs.stanford.edu/people/cbfinn/)
- **Samy Bengio** - Working in industry on generative models and ML robustness. [LinkedIn](https://www.linkedin.com/in/samybengio/) | [Website](https://scholar.google.com/citations?user=jMshKJYAAAAJ)
- **Jascha Sohl-Dickstein** - Working in industry on diffusion models and generative methods. [Website](https://jaschasd.github.io/)
- **Karen Simonyan** - Working in industry on vision-based reliability in deep learning. [LinkedIn](https://www.linkedin.com/in/karen-simonyan-75827a33/)
- **Christian Szegedy** - Working in industry on adversarial defenses and vision. [LinkedIn](https://www.linkedin.com/in/christian-szegedy-23a3459/)
- **Nick Bostrom** - Working in academia on AI safety and societal impacts. [Website](https://www.nickbostrom.com/)
- **Marta Kwiatkowska** - Working in academia on formal verification for AI safety. [Website](https://www.cs.ox.ac.uk/marta.kwiatkowska/)
- **Ryan Adams** - Working in academia on scalable Bayesian inference and uncertainty quantification. [Twitter](https://twitter.com/ryanadamsML) | [Website](https://www.cs.princeton.edu/~rpa/)
- **Katherine Heller** - Working in academia and industry on probabilistic modeling and Bayesian inference. [LinkedIn](https://www.linkedin.com/in/kheller27/)
- **Chris Olah** - Working in industry on interpretability and neural network reliability. [Twitter](https://twitter.com/ch402) | [Website](https://colah.github.io/)
- **Arvind Narayanan** - Working in academia on fairness, privacy, and secure ML systems. [Twitter](https://twitter.com/random_walker) | [Website](https://randomwalker.info/)
- **Joan Bruna** - Working in academia on robust and reliable graph neural networks. [Website](https://cims.nyu.edu/~bruna/)
- **Percy Liang** - Working in academia on generalization and ML reliability. [Twitter](https://twitter.com/percyliang) | [Website](https://cs.stanford.edu/people/pliang/)
- **Shakir Mohamed** - Working in industry on equitable and reliable AI systems. [Twitter](https://twitter.com/shakir_za) | [Website](https://shakirm.com/)
- **Michael Jordan** - Working in academia on probabilistic modeling and ML reliability. [Website](https://jordanlab.cs.berkeley.edu/)
- **Geoffrey Hinton** - Working in academia and industry on neural network reliability. [LinkedIn](https://www.linkedin.com/in/geoffrey-hinton/) | [Website](https://www.cs.toronto.edu/~hinton/)
- **Max Welling** - Working in academia on scalable Bayesian methods and ML uncertainty. [Twitter](https://twitter.com/wellingmax) | [Website](https://staff.fnwi.uva.nl/m.welling/)
- **Vladimir Vapnik** - Working in academia on generalization theories and SVMs. [Website](https://www.cs.columbia.edu/~vapnik/)
- **Leslie Kaelbling** - Working in academia on reliable reinforcement learning. [Website](https://www.lcs.mit.edu/leslie-kaelbling)
- **Pieter Abbeel** - Working in academia on robotics and real-world ML reliability. [Twitter](https://twitter.com/pabbeel) | [Website](https://people.eecs.berkeley.edu/~pabbeel/)
- **Sanja Fidler** - Working in academia on robust computer vision. [Website](https://www.cs.toronto.edu/~fidler/)
- **Thomas Dietterich** - Working in academia on fairness and reliability in ML. [Website](http://web.engr.oregonstate.edu/~tgd/)
- **Hanna Wallach** - Working in industry on societal impacts of ML and fairness. [Website](http://dirichlet.net/)
- **Anca Dragan** - Working in academia on reliable human-AI collaboration. [Website](https://people.eecs.berkeley.edu/~anca/)
- **David Silver** - Working in industry on reinforcement learning reliability. [LinkedIn](https://www.linkedin.com/in/david-silver-4b241/) | [Website](https://www.deepmind.com/research/highlighted-research/alphago)
- **Oriol Vinyals** - Working in industry on reliable multi-agent systems and RL. [LinkedIn](https://www.linkedin.com/in/oriol-vinyals-17721118/) | [Website](https://scholar.google.com/citations?user=ITRAiwIAAAAJ)
- **Kevin Murphy** - Working in academia and industry on probabilistic models and AI reliability. [Website](https://research.google/people/KevinMurphy/)
- **Cynthia Rudin** - Working in academia on interpretable and responsible AI. [Website](http://people.duke.edu/~cynthia/)
- **Pushmeet Kohli** - Working in industry on verifying ML robustness. [LinkedIn](https://www.linkedin.com/in/pushmeet/) | [Website](https://www.microsoft.com/en-us/research/people/pushmeet/)
- **Jonas Peters** - Working in academia on causal inference and reliable ML. [Website](https://jonas-peters.com/)
- **Adrian Weller** - Working in academia and industry on fairness and explainability. [Website](https://www.adrianweller.com/)

---

## Contributing ü§ù
We welcome contributions! Please follow our [contribution guidelines](CONTRIBUTING.md) to share resources, tools, or ideas that align with the theme of ML reliability.

---

## License üìÑ
This project is licensed under the [MIT License](LICENSE). Feel free to share and use the resources as needed.

---
