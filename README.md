# ğŸŒŸ Awesome ML Reliability ğŸŒŸ

![Awesome](https://awesome.re/badge.svg) ![MIT License](https://img.shields.io/badge/license-MIT-brightgreen)

A curated list of resources about Machine Learning (ML) reliability. It covers trustworthiness of ML systems by including resources on the topics listed below and more!

### Topics Summary ğŸ¨

| Topic            | Description                                    | Badge Example                                          |
|-------------------|------------------------------------------------|-------------------------------------------------------|
| **Uncertainty**   | Quantifying and managing uncertainty           | ![Uncertainty](https://img.shields.io/badge/Uncertainty-orange) |
| **Bayesian**      | Bayesian approaches to ML                      | ![Bayesian](https://img.shields.io/badge/Bayesian-lime) |
| **Robustness**    | Ensuring robustness to various conditions      | ![Robustness](https://img.shields.io/badge/Robustness-green) |
| **Attacks**       | Exploring adversarial attacks                  | ![Attacks](https://img.shields.io/badge/Attacks-red) |
| **Defense**       | Defense mechanisms against vulnerabilities     | ![Defense](https://img.shields.io/badge/Defense-blue) |
| **Privacy**       | Addressing privacy concerns in ML systems      | ![Privacy](https://img.shields.io/badge/Privacy-purple) |
| **Fairness**      | Ensuring fairness and equity in AI             | ![Fairness](https://img.shields.io/badge/Fairness-teal) |
| **Explainability**| Improving interpretability of AI systems       | ![Explainability](https://img.shields.io/badge/Explainability-cyan) |

If you find this list helpful, give it a â­ on GitHub, share it, and feel free to contribute by submitting a pull request or issue!

---

## Table of Contents
- [Facts/Numbers ğŸ“Š](#factsnumbers-ğŸ“Š)
- [Tools ğŸ› ï¸](#tools-ğŸ› ï¸)
- [Newspaper Articles ğŸ“°](#newspaper-articles-ğŸ“°)
- [Blog Articles ğŸ—’ï¸](#blog-articles-ğŸ—’ï¸)
- [Research Articles ğŸ•Œ](#research-articles-ğŸ•Œ)
- [Books ğŸ“š](#books-ğŸ“š)
- [Lectures ğŸ“](#lectures-ğŸ“)
- [People ğŸ§‘â€ğŸ’»](#people-ğŸ§‘â€ğŸ’»)

---

## Facts/Numbers ğŸ“Š
- **$1 Trillion**: Estimated annual global economic impact of unreliable AI systems ([Source](https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/the-state-of-ai-in-2023), 2023).
- **80%**: Percentage of ML projects that fail due to issues with data quality and system reliability ([Source](https://venturebeat.com/ai/mlops-failure-rates-in-machine-learning/), 2023).
- **60%**: Increase in regulatory frameworks targeting AI reliability since 2020 ([Source](https://www2.deloitte.com/insights/us/en/focus/tech-trends/2023/ai-governance.html), 2023).
- **3,000+**: Research papers on ML reliability published annually ([Source](https://www.semanticscholar.org/), 2022).

---

## Tools ğŸ› ï¸
- **[Uncertainty Baselines](https://github.com/google/uncertainty-baselines)**: High-quality implementations of standard and SOTA methods on a variety of tasks.
- **[Uncertainty Toolbox](https://uncertainty-toolbox.github.io/)**: A Python toolbox for predictive uncertainty quantification, calibration, metrics, and visualization.
- **[TensorFlow Model Analysis](https://www.tensorflow.org/tfx/guide/tfma)**: Framework for evaluating model fairness, robustness, and reliability.
- **[Cleanlab](https://github.com/cleanlab/cleanlab)**: A Python library for finding and fixing label errors in datasets.
- **[Robustness Gym](https://robustnessgym.com/)**: Tools for testing model performance across diverse scenarios.
- **[DeepChecks](https://github.com/deepchecks/deepchecks)**: Comprehensive library for testing ML systems and datasets.

---

## Newspaper Articles ğŸ“°
- *"The Dark Side of AI: How Reliability Issues Impact Society"* (2021) ![Robustness](https://img.shields.io/badge/Robustness-green) - [The New York Times](https://www.nytimes.com)
- *"Why Reliable AI is a Business Necessity"* (2022) ![Reliability](https://img.shields.io/badge/Reliability-blue) ![Business Impact](https://img.shields.io/badge/Business_Impact-orange) - [Forbes](https://www.forbes.com)
- *"Regulators are Closing In on AI Reliability"* (2023) ![Regulation](https://img.shields.io/badge/Regulation-red) ![Trustworthiness](https://img.shields.io/badge/Trustworthiness-purple) - [Financial Times](https://www.ft.com)

---

## Blog Articles ğŸ“
- *"How to Build Reliable AI Systems"* (2020) ![Reliability](https://img.shields.io/badge/Reliability-blue) ![Practical Applications](https://img.shields.io/badge/Practical_Applications-orange) - [Towards Data Science](https://towardsdatascience.com)
- *"Reliability in Machine Learning: Lessons from Industry"* (2021) ![Industry Practices](https://img.shields.io/badge/Industry_Practices-lime) ![Trustworthiness](https://img.shields.io/badge/Trustworthiness-purple) - [Google AI Blog](https://ai.googleblog.com)
- *"Top 10 Tools for ML Reliability Testing"* (2022) ![Tools](https://img.shields.io/badge/Tools-cyan) ![Testing](https://img.shields.io/badge/Testing-teal) ![Robustness](https://img.shields.io/badge/Robustness-green) - [Analytics Vidhya](https://www.analyticsvidhya.com)

---

## Research Articles ğŸ“„
- *"Adversarial Robustness for Deep Learning"* (2019) [Robustness, Security] - [ArXiv](https://arxiv.org/abs/1905.01065)
- *"Fairness and Robustness in AI Systems"* (2020) [Fairness, Robustness, Ethics] - [Nature Machine Intelligence](https://www.nature.com/natmachintell)
- *"Uncertainty Quantification in ML"* (2021) [Uncertainty, Reliability] - [IEEE Transactions on Neural Networks and Learning Systems](https://ieeexplore.ieee.org)
- *"Exploring Bayesian Methods in Uncertainty Quantification"* (2021) [Uncertainty, Bayesian Methods] - [Proceedings of Machine Learning Research](https://proceedings.mlr.press)
- *"Energy-based Models for Out-of-Distribution Detection"* (2022) [Robustness, OOD Detection] - [NeurIPS](https://neurips.cc)
- *"Graph Neural Networks: Robustness Under Adversarial Settings"* (2022) [Graph Learning, Robustness] - [ICLR](https://iclr.cc)

---

## Books ğŸ“š
- *"Reliable Machine Learning"* (2020) [Reliability, Engineering] by Erik Jones
- *"Trustworthy AI"* (2021) [Ethics, Trustworthiness] by Beena Ammanath
- *"Robust Machine Learning in Python"* (2022) [Robustness, Tools] by Sarah Bird
- *"Uncertainty in Deep Learning: A Practical Guide"* (2023) [Uncertainty, Methods] by Yarin Gal
- *"Explainable AI and its Reliability"* (2022) [Explainability, Reliability] by Christoph Molnar

---

## Lectures ğŸ“
- **"Reliability in AI: Challenges and Solutions"** [Reliability, Challenges] - [MIT OpenCourseWare](https://ocw.mit.edu)
- **"Robustness in Machine Learning"** [Robustness, Techniques] - [Stanford Online](https://online.stanford.edu)
- **"Trustworthy AI Systems"** [Trustworthiness, Systems] - [DeepMind YouTube](https://www.youtube.com)
- **"Uncertainty Estimation and Trust in ML"** [Uncertainty, Estimation] - [Cambridge University YouTube](https://www.youtube.com)

---

## People ğŸ§‘â€ğŸ’»
- **Bertrand Charpentier** - Working in academia and industry on Bayesian deep learning and uncertainty estimation. [LinkedIn](https://www.linkedin.com/in/bertrand-charpentier/)
- **Stephan GÃ¼nnemann** - Working in academia on robust machine learning and graph neural networks. [Website](https://www.professoren.tum.de/guennemann-stephan)
- **Eyke HÃ¼llermeier** - Working in academia on uncertainty quantification and decision theory. [Website](https://www.uni-paderborn.de/en/person/16348)
- **Eric Nalisnick** - Working in academia on Bayesian deep learning and uncertainty estimation. [Website](https://www.cs.nott.ac.uk/~ean/)
- **Andrew Ng** - Working in academia and industry on online education and ML reliability. [Twitter](https://twitter.com/andrewyng) | [LinkedIn](https://www.linkedin.com/in/andrewyng/) | [Website](https://www.andrewng.org/)
- **Fei-Fei Li** - Working in academia on AI ethics and vision. [Twitter](https://twitter.com/feifeili) | [LinkedIn](https://www.linkedin.com/in/feifei-li/) | [Website](https://profiles.stanford.edu/fei-fei-li)
- **Timnit Gebru** - Working in industry on fairness and ethical AI. [Twitter](https://twitter.com/timnitGebru) | [Website](https://www.dair-institute.org/)
- **Ian Goodfellow** - Working in industry on adversarial robustness and ML security. [LinkedIn](https://www.linkedin.com/in/ian-goodfellow/) | [Website](https://www.iangoodfellow.com/)
- **Yarin Gal** - Working in academia on Bayesian methods and predictive uncertainty. [Twitter](https://twitter.com/yarin_gal) | [Website](https://yarin-gal.com/)
- **Zoubin Ghahramani** - Working in industry and academia on probabilistic models and Bayesian methods. [Twitter](https://twitter.com/zoubinghahramani) | [Website](https://mlg.eng.cam.ac.uk/zoubin/)
- **Balaji Lakshminarayanan** - Working in industry on deep ensembles and predictive uncertainty. [LinkedIn](https://www.linkedin.com/in/balaji-lakshminarayanan/) | [Website](https://balajiln.github.io/)
- **Dan Hendrycks** - Working in academia on robustness to distribution shifts and ML reliability. [Website](https://hendrycks.com/)
- **Alexander Amini** - Working in academia on evidential learning and autonomous systems. [Website](https://people.csail.mit.edu/amini/)
- **Dario Amodei** - Working in industry on AI alignment and safety. [LinkedIn](https://www.linkedin.com/in/darioamodei/)
- **Nicholas Carlini** - Working in industry on adversarial robustness and secure ML. [Website](https://nicholas.carlini.com/)
- **Chelsea Finn** - Working in academia on meta-learning and reliability in robotics. [Twitter](https://twitter.com/chelseabfinn) | [Website](https://cs.stanford.edu/people/cbfinn/)
- **Samy Bengio** - Working in industry on generative models and ML robustness. [LinkedIn](https://www.linkedin.com/in/samybengio/) | [Website](https://scholar.google.com/citations?user=jMshKJYAAAAJ)
- **Jascha Sohl-Dickstein** - Working in industry on diffusion models and generative methods. [Website](https://jaschasd.github.io/)
- **Karen Simonyan** - Working in industry on vision-based reliability in deep learning. [LinkedIn](https://www.linkedin.com/in/karen-simonyan-75827a33/)
- **Christian Szegedy** - Working in industry on adversarial defenses and vision. [LinkedIn](https://www.linkedin.com/in/christian-szegedy-23a3459/)
- **Nick Bostrom** - Working in academia on AI safety and societal impacts. [Website](https://www.nickbostrom.com/)
- **Marta Kwiatkowska** - Working in academia on formal verification for AI safety. [Website](https://www.cs.ox.ac.uk/marta.kwiatkowska/)
- **Ryan Adams** - Working in academia on scalable Bayesian inference and uncertainty quantification. [Twitter](https://twitter.com/ryanadamsML) | [Website](https://www.cs.princeton.edu/~rpa/)
- **Katherine Heller** - Working in academia and industry on probabilistic modeling and Bayesian inference. [LinkedIn](https://www.linkedin.com/in/kheller27/)
- **Chris Olah** - Working in industry on interpretability and neural network reliability. [Twitter](https://twitter.com/ch402) | [Website](https://colah.github.io/)
- **Arvind Narayanan** - Working in academia on fairness, privacy, and secure ML systems. [Twitter](https://twitter.com/random_walker) | [Website](https://randomwalker.info/)
- **Joan Bruna** - Working in academia on robust and reliable graph neural networks. [Website](https://cims.nyu.edu/~bruna/)
- **Percy Liang** - Working in academia on generalization and ML reliability. [Twitter](https://twitter.com/percyliang) | [Website](https://cs.stanford.edu/people/pliang/)
- **Shakir Mohamed** - Working in industry on equitable and reliable AI systems. [Twitter](https://twitter.com/shakir_za) | [Website](https://shakirm.com/)
- **Michael Jordan** - Working in academia on probabilistic modeling and ML reliability. [Website](https://jordanlab.cs.berkeley.edu/)
- **Geoffrey Hinton** - Working in academia and industry on neural network reliability. [LinkedIn](https://www.linkedin.com/in/geoffrey-hinton/) | [Website](https://www.cs.toronto.edu/~hinton/)
- **Max Welling** - Working in academia on scalable Bayesian methods and ML uncertainty. [Twitter](https://twitter.com/wellingmax) | [Website](https://staff.fnwi.uva.nl/m.welling/)
- **Vladimir Vapnik** - Working in academia on generalization theories and SVMs. [Website](https://www.cs.columbia.edu/~vapnik/)
- **Leslie Kaelbling** - Working in academia on reliable reinforcement learning. [Website](https://www.lcs.mit.edu/leslie-kaelbling)
- **Pieter Abbeel** - Working in academia on robotics and real-world ML reliability. [Twitter](https://twitter.com/pabbeel) | [Website](https://people.eecs.berkeley.edu/~pabbeel/)
- **Sanja Fidler** - Working in academia on robust computer vision. [Website](https://www.cs.toronto.edu/~fidler/)
- **Thomas Dietterich** - Working in academia on fairness and reliability in ML. [Website](http://web.engr.oregonstate.edu/~tgd/)
- **Hanna Wallach** - Working in industry on societal impacts of ML and fairness. [Website](http://dirichlet.net/)
- **Anca Dragan** - Working in academia on reliable human-AI collaboration. [Website](https://people.eecs.berkeley.edu/~anca/)
- **David Silver** - Working in industry on reinforcement learning reliability. [LinkedIn](https://www.linkedin.com/in/david-silver-4b241/) | [Website](https://www.deepmind.com/research/highlighted-research/alphago)
- **Oriol Vinyals** - Working in industry on reliable multi-agent systems and RL. [LinkedIn](https://www.linkedin.com/in/oriol-vinyals-17721118/) | [Website](https://scholar.google.com/citations?user=ITRAiwIAAAAJ)
- **Kevin Murphy** - Working in academia and industry on probabilistic models and AI reliability. [Website](https://research.google/people/KevinMurphy/)
- **Cynthia Rudin** - Working in academia on interpretable and responsible AI. [Website](http://people.duke.edu/~cynthia/)
- **Pushmeet Kohli** - Working in industry on verifying ML robustness. [LinkedIn](https://www.linkedin.com/in/pushmeet/) | [Website](https://www.microsoft.com/en-us/research/people/pushmeet/)
- **Jonas Peters** - Working in academia on causal inference and reliable ML. [Website](https://jonas-peters.com/)
- **Adrian Weller** - Working in academia and industry on fairness and explainability. [Website](https://www.adrianweller.com/)

---

## Contributing ğŸ¤
We welcome contributions! Please follow our [contribution guidelines](CONTRIBUTING.md) to share resources, tools, or ideas that align with the theme of ML reliability.

---

## License ğŸ“„
This project is licensed under the [MIT License](LICENSE). Feel free to share and use the resources as needed.

---
